"""
Prepare probability predictions and ground truth labels for calibration/threshold tuning.

Supports predictions generated by txr_infer.py (prob_* columns) as well as any file
that already contains per-label probabilities.

Output format matches the expectation of fit_label_calibrators.py and threshold_tuner.py
with columns y_true_<LABEL>, y_pred_<LABEL>.
"""

from __future__ import annotations

import argparse
from pathlib import Path
from typing import Dict, List, Optional

import numpy as np
import pandas as pd

CHEXPERT13 = [
    "Enlarged Cardiomediastinum",
    "Cardiomegaly",
    "Lung Opacity",
    "Lung Lesion",
    "Edema",
    "Consolidation",
    "Pneumonia",
    "Atelectasis",
    "Pneumothorax",
    "Pleural Effusion",
    "Pleural Other",
    "Fracture",
    "Support Devices",
]

CHEXPERT14 = CHEXPERT13 + ["No Finding"]


def find_prediction_column(df: pd.DataFrame, label: str, score_prefixes: List[str]) -> Optional[str]:
    """Return the column name containing scores for a label."""
    candidates = []
    for prefix in score_prefixes:
        candidates.append(f"{prefix}{label}")
        candidates.append(f"{prefix}{label.replace(' ', '_')}")
    candidates.extend([
        label,
        label.replace(" ", "_"),
        label.lower().replace(" ", "_"),
    ])
    for cand in candidates:
        if cand in df.columns:
            return cand
    return None


def prepare_predictions(
    predictions_csv: Path,
    ground_truth_csv: Path,
    output_csv: Path,
    score_prefixes: List[str],
    drop_na: bool = True,
) -> pd.DataFrame:
    pred_df = pd.read_csv(predictions_csv)
    gt_df = pd.read_csv(ground_truth_csv)

    # Ensure filename column exists (handle both "image" and "filename" columns)
    if "filename" not in pred_df.columns:
        if "image" in pred_df.columns:
            pred_df["filename"] = pred_df["image"].apply(lambda x: Path(x).name if pd.notna(x) else "")
        else:
            raise ValueError(f"Neither 'filename' nor 'image' column found in {predictions_csv}")
    else:
        # If filename exists but is full path, extract just the name
        if pred_df["filename"].str.contains("/").any():
            pred_df["filename"] = pred_df["filename"].apply(lambda x: Path(x).name if pd.notna(x) else "")
    
    if "filename" not in gt_df.columns:
        if "image" in gt_df.columns:
            gt_df["filename"] = gt_df["image"].apply(lambda x: Path(x).name if pd.notna(x) else "")
        else:
            raise ValueError(f"Neither 'filename' nor 'image' column found in {ground_truth_csv}")
    else:
        # If filename exists but is full path, extract just the name
        if gt_df["filename"].str.contains("/").any():
            gt_df["filename"] = gt_df["filename"].apply(lambda x: Path(x).name if pd.notna(x) else "")

    merged = pd.merge(pred_df, gt_df, on="filename", suffixes=("_pred", "_gt"))
    if merged.empty:
        raise ValueError("No overlapping images between predictions and ground truth.")

    output: Dict[str, np.ndarray] = {"filename": merged["filename"].values}

    for label in CHEXPERT14:
        # Skip "No Finding" if not present (it's derived)
        if label == "No Finding":
            # No Finding is derived from other labels, skip it here
            continue
            
        pred_col = find_prediction_column(merged, label, score_prefixes=score_prefixes)
        if pred_col is None:
            print(f"⚠️  Skipping '{label}': probability column not found")
            continue
        series = merged[pred_col].astype(float)
        if series.isna().all():
            print(f"⚠️  All predictions for '{label}' are NaN. Filling with zeros.")
            series = series.fillna(0.0)
        else:
            series = series.fillna(0.0)
        output[f"y_pred_{label}"] = series.values

        gt_col = f"{label}_gt"
        if gt_col not in merged.columns:
            alt = label
            if alt not in merged.columns:
                print(f"⚠️  Skipping '{label}': ground truth column not found")
                continue
            gt_col = alt
        gt_values = pd.to_numeric(merged[gt_col], errors="coerce")
        output[f"y_true_{label}"] = gt_values.values

    df = pd.DataFrame(output)
    if drop_na:
        before = len(df)
        df = df.dropna()
        if len(df) < before:
            print(f"⚠️  Dropped {before - len(df)} rows with NaN predictions.")

    df.to_csv(output_csv, index=False)
    print(f"✅ Saved {len(df)} rows to {output_csv}")
    return df


def main():
    parser = argparse.ArgumentParser(description="Prepare predictions for calibration / threshold tuning")
    parser.add_argument("--predictions", required=True, help="CSV containing probabilities (e.g., txr_predictions.csv)")
    parser.add_argument("--ground_truth", required=True, help="Ground truth CSV")
    parser.add_argument("--output", default="calibration_input.csv", help="Output CSV path")
    parser.add_argument(
        "--score_prefix",
        action="append",
        default=["prob_", "y_pred_", "y_cal_"],
        help="Column prefix(es) for probability scores (default: prob_, y_pred_, y_cal_). "
             "Use multiple --score_prefix flags to specify additional prefixes.",
    )
    parser.add_argument("--keep_na", action="store_true", help="Keep rows with NaN predictions")
    args = parser.parse_args()

    prepare_predictions(
        predictions_csv=Path(args.predictions),
        ground_truth_csv=Path(args.ground_truth),
        output_csv=Path(args.output),
        score_prefixes=args.score_prefix,
        drop_na=not args.keep_na,
    )


if __name__ == "__main__":
    main()
